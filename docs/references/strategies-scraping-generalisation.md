# ğŸ•·ï¸ StratÃ©gies de Scraping et GÃ©nÃ©ralisation Ã  Tout le Site

**Date :** 2025-01-XX  
**Objectif :** Documenter toutes les stratÃ©gies employÃ©es pour rÃ©cupÃ©rer les informations cachÃ©es et proposer un plan pour gÃ©nÃ©raliser I-Amiens Ã  tout le site amiens.fr

---

## ğŸ“‹ Table des MatiÃ¨res

1. [StratÃ©gies Actuelles](#stratÃ©gies-actuelles)
2. [Architecture de DÃ©couverte](#architecture-de-dÃ©couverte)
3. [Plan de GÃ©nÃ©ralisation](#plan-de-gÃ©nÃ©ralisation)
4. [ImplÃ©mentation](#implÃ©mentation)

---

## ğŸ¯ StratÃ©gies Actuelles

### 1. **DÃ©couverte par Push-Blocks (H2 â†’ URLs)**

**Fichier :** `tools/rebuild_corpus.py`  
**Fonction :** `discover_push_blocks()`

**Principe :**
- Le site amiens.fr utilise des blocs de navigation avec la classe `.push-block__inner`
- Chaque bloc contient un `<h2>` qui reprÃ©sente une section
- Le texte du H2 peut Ãªtre converti en slug pour construire une URL

**Code actuel :**
```python
def discover_push_blocks(soup: BeautifulSoup, base_url: str) -> List[str]:
    urls = []
    blocks = soup.select(".push-block__inner h2")
    for block in blocks:
        text = block.get_text(strip=True)
        if not text:
            continue
        slug = slugify(text)  # Convertit en slug (minuscules, tirets)
        candidate = f"{base_url.rstrip('/')}/{slug}"
        urls.append(candidate)
    return urls
```

**Exemple :**
- H2 : "ModalitÃ©s d'inscription"
- Slug : `modalites-d-inscription`
- URL construite : `https://www.amiens.fr/vivre-a-amiens/enfance/centres-de-loisirs/modalites-d-inscription`

**Limitations actuelles :**
- âŒ Fonctionne seulement pour la section Enfance
- âŒ Ne gÃ¨re pas les variations de casse dans les URLs
- âŒ Ne vÃ©rifie pas si l'URL existe avant de l'ajouter

---

### 2. **Scraping Dynamique avec Playwright**

**Fichier :** `ML/scripts/Audit_Scrap_enfance.py`

**StratÃ©gies :**

#### a) **Clics automatiques sur Ã©lÃ©ments interactifs**
```python
# Cliquer sur tous les "voir +" et accordÃ©ons
await page.evaluate("""
    () => {
        document.querySelectorAll('button, a').forEach(el => {
            if(/voir|plus|dÃ©tails/i.test(el.innerText)) el.click();
        });
    }
""")
```

#### b) **Extraction de contenu cachÃ©**
- Tables avec `display:none`
- Ã‰lÃ©ments `aria-hidden="true"`
- Contenu dans accordÃ©ons fermÃ©s

#### c) **DÃ©couverte rÃ©cursive de liens**
- Suit tous les liens internes
- Limite Ã  150 pages par dÃ©faut
- Filtre par domaine (`BASE_URL`)

**Avantages :**
- âœ… RÃ©cupÃ¨re le contenu gÃ©nÃ©rÃ© par JavaScript
- âœ… DÃ©couvre les pages cachÃ©es dans les accordÃ©ons
- âœ… Extrait les PDFs automatiquement

**Limitations :**
- âŒ Lent (nÃ©cessite un navigateur)
- âŒ LimitÃ© Ã  la section Enfance (`BASE_URL = "https://www.amiens.fr/Vivre-a-Amiens/Enfance"`)

---

### 3. **Extraction de PDFs**

**Fichier :** `tools/rebuild_corpus.py`  
**Fonction :** `extract_pdf_links()`

**Principe :**
```python
def extract_pdf_links(soup: BeautifulSoup) -> List[str]:
    pdf_links = []
    for tag in soup.select("a[href$='.pdf']"):
        href = tag.get("href")
        if href:
            if href.startswith("http"):
                pdf_links.append(href)
            else:
                pdf_links.append("https://www.amiens.fr" + href)
    return pdf_links
```

**Utilisation :**
- Les PDFs sont indexÃ©s comme segments sÃ©parÃ©s
- CatÃ©gorie : `{category}_pdf`
- Contenu : URL du PDF (peut Ãªtre enrichi avec extraction texte)

---

### 4. **Crawling Respectueux (robots.txt)**

**Fichier :** `ML/scripts/# crawler_respectueux.py`

**Principe :**
- VÃ©rifie `robots.txt` avant chaque requÃªte
- Respecte les dÃ©lais entre requÃªtes (1 seconde)
- Filtre par section (`/Enfance`)

**Code :**
```python
rp = urllib.robotparser.RobotFileParser()
rp.set_url(urljoin(BASE, "/robots.txt"))
rp.read()
if not rp.can_fetch(USER_AGENT, url):
    continue
```

---

## ğŸ—ï¸ Architecture de DÃ©couverte

### StratÃ©gies Multi-Niveaux

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Niveau 1 : URLs Directes (Sources ConfigurÃ©es) â”‚
â”‚  - URLs dans corpus_sources.json                â”‚
â”‚  - URLs par dÃ©faut (DEFAULT_SOURCES)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Niveau 2 : DÃ©couverte Push-Blocks (H2)        â”‚
â”‚  - Extraire H2 de .push-block__inner           â”‚
â”‚  - Slugifier et construire URLs                â”‚
â”‚  - VÃ©rifier existence (HEAD request)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Niveau 3 : Suivi Liens Internes               â”‚
â”‚  - Extraire tous les <a href>                  â”‚
â”‚  - Filtrer par domaine amiens.fr               â”‚
â”‚  - Filtrer par pattern (optionnel)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Niveau 4 : Scraping Dynamique (Playwright)    â”‚
â”‚  - Cliquer sur Ã©lÃ©ments interactifs            â”‚
â”‚  - Extraire contenu cachÃ© (display:none)      â”‚
â”‚  - DÃ©couvrir nouvelles URLs aprÃ¨s interaction  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Plan de GÃ©nÃ©ralisation

### Phase 1 : GÃ©nÃ©raliser les StratÃ©gies de DÃ©couverte

#### 1.1. **Fonction de DÃ©couverte Universelle**

**CrÃ©er :** `tools/discover_urls.py`

```python
"""
Module de dÃ©couverte d'URLs pour amiens.fr
GÃ©nÃ©ralise les stratÃ©gies de dÃ©couverte Ã  tout le site
"""

from typing import List, Set, Optional
from urllib.parse import urljoin, urlparse
import requests
from bs4 import BeautifulSoup

class URLDiscoverer:
    def __init__(self, base_domain: str = "https://www.amiens.fr"):
        self.base_domain = base_domain
        self.visited: Set[str] = set()
    
    def discover_push_blocks(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """DÃ©couvre les URLs via les push-blocks (H2)"""
        urls = []
        blocks = soup.select(".push-block__inner h2")
        for block in blocks:
            text = block.get_text(strip=True)
            if not text:
                continue
            slug = slugify(text)
            candidate = f"{base_url.rstrip('/')}/{slug}"
            if self._is_valid_url(candidate):
                urls.append(candidate)
        return urls
    
    def discover_internal_links(self, soup: BeautifulSoup, 
                                current_url: str,
                                pattern: Optional[str] = None) -> List[str]:
        """DÃ©couvre les URLs via les liens internes"""
        urls = []
        for link in soup.select("a[href]"):
            href = link.get("href")
            if not href:
                continue
            absolute_url = urljoin(current_url, href)
            parsed = urlparse(absolute_url)
            
            # Filtrer par domaine
            if parsed.netloc != urlparse(self.base_domain).netloc:
                continue
            
            # Filtrer par pattern si fourni
            if pattern and pattern not in absolute_url:
                continue
            
            # Enlever fragments et query params pour normaliser
            canonical = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            if canonical not in self.visited:
                urls.append(canonical)
        
        return urls
    
    def discover_from_navigation(self, soup: BeautifulSoup, 
                                 base_url: str) -> List[str]:
        """DÃ©couvre les URLs depuis les menus de navigation"""
        urls = []
        
        # Navigation principale
        nav_links = soup.select("nav a[href], .navigation a[href]")
        for link in nav_links:
            href = link.get("href")
            if href:
                absolute_url = urljoin(base_url, href)
                if self._is_valid_url(absolute_url):
                    urls.append(absolute_url)
        
        return urls
    
    def discover_from_sitemap(self) -> List[str]:
        """DÃ©couvre les URLs depuis le sitemap.xml"""
        urls = []
        sitemap_url = f"{self.base_domain}/sitemap.xml"
        try:
            resp = requests.get(sitemap_url, timeout=10)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.content, "xml")
                for loc in soup.find_all("loc"):
                    url = loc.text.strip()
                    if url.startswith(self.base_domain):
                        urls.append(url)
        except Exception as e:
            print(f"âš ï¸ Erreur sitemap: {e}")
        
        return urls
    
    def _is_valid_url(self, url: str) -> bool:
        """VÃ©rifie si l'URL existe (HEAD request)"""
        if url in self.visited:
            return False
        try:
            resp = requests.head(url, timeout=5, allow_redirects=True)
            if resp.status_code == 200:
                self.visited.add(url)
                return True
        except:
            pass
        return False
```

---

#### 1.2. **Configuration par Section**

**CrÃ©er :** `ML/data/site_sections.json`

```json
{
  "sections": [
    {
      "name": "Enfance",
      "base_url": "https://www.amiens.fr/Vivre-a-Amiens/Enfance",
      "pattern": "/Enfance",
      "categories": ["tarifs", "centres_loisirs", "inscriptions", "menus", "rpe"]
    },
    {
      "name": "Jeunesse",
      "base_url": "https://www.amiens.fr/Vivre-a-Amiens/Jeunesse",
      "pattern": "/Jeunesse",
      "categories": ["activites", "inscriptions", "tarifs"]
    },
    {
      "name": "Culture",
      "base_url": "https://www.amiens.fr/Vivre-a-Amiens/Culture",
      "pattern": "/Culture",
      "categories": ["evenements", "lieux", "reservations"]
    },
    {
      "name": "Sport",
      "base_url": "https://www.amiens.fr/Vivre-a-Amiens/Sport",
      "pattern": "/Sport",
      "categories": ["equipements", "inscriptions", "tarifs"]
    }
  ]
}
```

---

### Phase 2 : Crawler Multi-Section

#### 2.1. **Crawler GÃ©nÃ©ralisÃ©**

**CrÃ©er :** `tools/crawl_site.py`

```python
#!/usr/bin/env python3
"""
Crawler gÃ©nÃ©ralisÃ© pour tout le site amiens.fr
Utilise toutes les stratÃ©gies de dÃ©couverte
"""

import json
from pathlib import Path
from typing import List, Dict
from discover_urls import URLDiscoverer
from rebuild_corpus import parse_page, Segment

def crawl_section(section_config: Dict, discoverer: URLDiscoverer) -> List[Segment]:
    """Crawl une section complÃ¨te du site"""
    base_url = section_config["base_url"]
    pattern = section_config.get("pattern")
    categories = section_config.get("categories", [])
    
    all_segments = []
    to_visit = [base_url]
    visited = set()
    
    while to_visit:
        url = to_visit.pop(0)
        if url in visited:
            continue
        visited.add(url)
        
        try:
            # Parser la page
            segments = parse_page(url, categories[0] if categories else "general")
            all_segments.extend(segments)
            
            # DÃ©couvrir de nouvelles URLs
            soup = fetch_page(url)
            
            # StratÃ©gie 1: Push-blocks
            push_urls = discoverer.discover_push_blocks(soup, url)
            for new_url in push_urls:
                if new_url not in visited:
                    to_visit.append(new_url)
            
            # StratÃ©gie 2: Liens internes
            internal_urls = discoverer.discover_internal_links(soup, url, pattern)
            for new_url in internal_urls:
                if new_url not in visited:
                    to_visit.append(new_url)
            
            # StratÃ©gie 3: Navigation
            nav_urls = discoverer.discover_from_navigation(soup, url)
            for new_url in nav_urls:
                if new_url not in visited:
                    to_visit.append(new_url)
        
        except Exception as e:
            print(f"âš ï¸ Erreur sur {url}: {e}")
            continue
    
    return all_segments

def crawl_all_sections(config_path: str) -> List[Segment]:
    """Crawl toutes les sections configurÃ©es"""
    with open(config_path) as f:
        config = json.load(f)
    
    discoverer = URLDiscoverer()
    all_segments = []
    
    # StratÃ©gie 0: Sitemap
    sitemap_urls = discoverer.discover_from_sitemap()
    print(f"âœ… {len(sitemap_urls)} URLs dÃ©couvertes via sitemap")
    
    for section in config["sections"]:
        print(f"\nğŸ” Crawling section: {section['name']}")
        segments = crawl_section(section, discoverer)
        all_segments.extend(segments)
        print(f"âœ… {len(segments)} segments trouvÃ©s")
    
    return all_segments
```

---

### Phase 3 : Scraping Dynamique GÃ©nÃ©ralisÃ©

#### 3.1. **Playwright Multi-Section**

**Modifier :** `ML/scripts/Audit_Scrap_enfance.py` â†’ `ML/scripts/crawl_dynamic.py`

```python
"""
Scraping dynamique gÃ©nÃ©ralisÃ© avec Playwright
GÃ©nÃ©ralise Ã  toutes les sections du site
"""

async def crawl_section_dynamic(section_config: Dict, page):
    """Crawl dynamique d'une section"""
    base_url = section_config["base_url"]
    pattern = section_config.get("pattern")
    
    visited = set()
    to_visit = [base_url]
    
    while to_visit and len(visited) < MAX_PAGES:
        url = to_visit.pop(0)
        if url in visited:
            continue
        visited.add(url)
        
        try:
            await page.goto(url, timeout=20000)
            
            # Cliquer sur Ã©lÃ©ments interactifs
            await page.evaluate("""
                () => {
                    // Cliquer sur "voir +", accordÃ©ons, etc.
                    document.querySelectorAll('button, a, [role="button"]').forEach(el => {
                        const text = el.innerText?.toLowerCase() || '';
                        if(/voir|plus|dÃ©tails|afficher|dÃ©velopper/i.test(text)) {
                            el.click();
                        }
                    });
                    
                    // Ouvrir tous les accordÃ©ons
                    document.querySelectorAll('[aria-expanded="false"]').forEach(el => {
                        el.click();
                    });
                }
            """)
            
            await page.wait_for_timeout(1000)
            await page.wait_for_load_state("networkidle")
            
            # Extraire contenu
            content = await page.content()
            await save_file(f"{slugify(url)}.html", content)
            
            # DÃ©couvrir nouveaux liens
            new_links = await page.locator("a[href]").evaluate_all(
                "els => els.map(el => el.href)"
            )
            
            for link in new_links:
                if link and pattern in link and link not in visited:
                    to_visit.append(link)
        
        except Exception as e:
            print(f"âš ï¸ {url}: {e}")
```

---

## ğŸ”§ ImplÃ©mentation

### Ã‰tape 1 : CrÃ©er les Modules de DÃ©couverte

```bash
cd "I Amiens"
mkdir -p tools/discovery
touch tools/discovery/__init__.py
touch tools/discovery/url_discoverer.py
touch tools/discovery/section_crawler.py
```

### Ã‰tape 2 : GÃ©nÃ©raliser `rebuild_corpus.py`

**Modifier :** `tools/rebuild_corpus.py`

- Ajouter support multi-sections
- Utiliser `URLDiscoverer` pour toutes les stratÃ©gies
- Charger configuration depuis `site_sections.json`

### Ã‰tape 3 : CrÃ©er Script de Crawl Complet

**CrÃ©er :** `tools/crawl_full_site.py`

```python
#!/usr/bin/env python3
"""
Crawl complet du site amiens.fr
Combine toutes les stratÃ©gies de dÃ©couverte
"""

from discovery.url_discoverer import URLDiscoverer
from discovery.section_crawler import crawl_all_sections
from rebuild_corpus import rebuild_corpus

def main():
    # 1. Crawl statique (BeautifulSoup)
    print("ğŸ” Phase 1: Crawl statique...")
    corpus_static = rebuild_corpus()  # ModifiÃ© pour multi-sections
    
    # 2. Crawl dynamique (Playwright) - optionnel
    print("\nğŸ” Phase 2: Crawl dynamique...")
    # corpus_dynamic = crawl_dynamic_all_sections()
    
    # 3. Fusion et dÃ©duplication
    print("\nâœ… Fusion des corpus...")
    # corpus_final = merge_corpus(corpus_static, corpus_dynamic)
    
    print(f"âœ… Corpus final: {len(corpus_final)} segments")

if __name__ == "__main__":
    main()
```

---

## ğŸ“Š StratÃ©gies RÃ©sumÃ©es

| StratÃ©gie | Fichier Actuel | GÃ©nÃ©ralisation | PrioritÃ© |
|-----------|----------------|-----------------|----------|
| **Push-Blocks (H2)** | `rebuild_corpus.py` | âœ… `discover_urls.py` | Haute |
| **Liens Internes** | `Audit_Scrap_enfance.py` | âœ… `discover_urls.py` | Haute |
| **Scraping Dynamique** | `Audit_Scrap_enfance.py` | âœ… `crawl_dynamic.py` | Moyenne |
| **Sitemap** | âŒ Non implÃ©mentÃ© | âœ… `discover_urls.py` | Basse |
| **Navigation** | âŒ Non implÃ©mentÃ© | âœ… `discover_urls.py` | Basse |
| **PDFs** | `rebuild_corpus.py` | âœ… DÃ©jÃ  gÃ©nÃ©ralisÃ© | Haute |

---

## ğŸ¯ Checklist de GÃ©nÃ©ralisation

- [ ] CrÃ©er `tools/discovery/url_discoverer.py`
- [ ] CrÃ©er `ML/data/site_sections.json`
- [ ] Modifier `tools/rebuild_corpus.py` pour multi-sections
- [ ] CrÃ©er `tools/crawl_full_site.py`
- [ ] GÃ©nÃ©raliser `ML/scripts/Audit_Scrap_enfance.py` â†’ `crawl_dynamic.py`
- [ ] Tester sur section Enfance (rÃ©gression)
- [ ] Tester sur section Jeunesse
- [ ] Tester sur section Culture
- [ ] Documenter les nouvelles URLs dÃ©couvertes
- [ ] Mettre Ã  jour `corpus_sources.json` avec nouvelles sections

---

## ğŸ“ Notes Importantes

### Limitations Ã  Garder en TÃªte

1. **Rate Limiting** : Respecter `robots.txt` et dÃ©lais entre requÃªtes
2. **Contenu Dynamique** : Certaines pages nÃ©cessitent JavaScript (Playwright)
3. **URLs Variables** : Certaines URLs peuvent changer (versions, dates)
4. **Contenu ProtÃ©gÃ©** : Certaines pages peuvent nÃ©cessiter authentification

### AmÃ©liorations Futures

1. **Cache** : Mettre en cache les pages dÃ©jÃ  crawlÃ©
2. **ParallÃ©lisation** : Crawler plusieurs sections en parallÃ¨le
3. **Monitoring** : Logger les URLs dÃ©couvertes vs visitÃ©es
4. **Validation** : VÃ©rifier qualitÃ© du contenu extrait

---

**DerniÃ¨re mise Ã  jour :** 2025-01-XX

