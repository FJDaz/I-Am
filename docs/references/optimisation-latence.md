# Optimisation de la Latence du Serveur Local

## üîç Analyse des Causes de Latence

### Causes identifi√©es :
1. **Pas de compression HTTP** (gzip/brotli)
2. **Timeout trop long** (60s pour Claude API)
3. **Pas de workers multiples** (uvicorn single-threaded)
4. **Chargements synchrones** au d√©marrage
5. **Pas de cache** pour requ√™tes r√©p√©t√©es
6. **Pas de connexions keep-alive optimis√©es**

## üöÄ Solutions d'Optimisation

### 1. Compression HTTP (Gzip)

**Impact** : R√©duction de 60-80% de la taille des r√©ponses

```python
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)
```

**Ajouter dans `rag_assistant_server.py` apr√®s ligne 965** :
```python
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)
```

### 2. R√©duire le Timeout Claude API

**Impact** : √âvite les attentes inutiles, fail-fast

**Modifier ligne 921** :
```python
timeout=30.0,  # Au lieu de 60.0
```

### 3. Workers Multiples (Uvicorn)

**Impact** : Traitement parall√®le de plusieurs requ√™tes

**Modifier les lignes 1296-1298** :
```python
if os.path.exists(ssl_keyfile) and os.path.exists(ssl_certfile):
    uvicorn.run(
        "rag_assistant_server:app",
        host="0.0.0.0",
        port=8711,
        ssl_keyfile=ssl_keyfile,
        ssl_certfile=ssl_certfile,
        workers=2,  # 2 workers pour d√©veloppement local
        loop="asyncio"
    )
else:
    uvicorn.run(
        "rag_assistant_server:app",
        host="0.0.0.0",
        port=8711,
        workers=2,
        loop="asyncio"
    )
```

**‚ö†Ô∏è Attention** : Les workers multiples n√©cessitent que les donn√©es (embeddings, metadata) soient partag√©es en m√©moire ou recharg√©es par worker.

### 4. Cache Simple pour Requ√™tes Fr√©quentes

**Impact** : R√©ponses instantan√©es pour questions identiques

```python
from functools import lru_cache
from hashlib import md5
import json

# Cache simple en m√©moire (max 100 entr√©es)
_request_cache = {}
CACHE_TTL = 300  # 5 minutes

def get_cache_key(payload: AssistantRequest) -> str:
    """G√©n√®re une cl√© de cache bas√©e sur la question"""
    key_data = {
        "question": payload.question,
        "normalized": payload.normalized_question,
        "rag_count": len(payload.rag_results or [])
    }
    return md5(json.dumps(key_data, sort_keys=True).encode()).hexdigest()

# Dans rag_assistant_endpoint, avant le traitement :
cache_key = get_cache_key(payload)
if cache_key in _request_cache:
    cached_response, cached_time = _request_cache[cache_key]
    if time.time() - cached_time < CACHE_TTL:
        return cached_response
```

### 5. Chargement Asynchrone des Donn√©es

**Impact** : D√©marrage plus rapide, chargement en arri√®re-plan

```python
import asyncio
from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: charger en arri√®re-plan
    asyncio.create_task(load_data_async())
    yield
    # Shutdown: cleanup si n√©cessaire
    pass

app = FastAPI(
    title="RAG Assistant Amiens V2",
    version="0.2.0",
    lifespan=lifespan
)
```

### 6. Optimiser les Appels API Claude

**Impact** : R√©duction du temps de r√©ponse

- **Streaming** : Si possible, utiliser streaming pour afficher la r√©ponse progressivement
- **Max tokens** : Limiter `max_tokens` pour √©viter les r√©ponses trop longues
- **Temperature** : R√©duire si pas n√©cessaire (0.3 au lieu de 0.7)

### 7. Connexions Keep-Alive

**Impact** : R√©utilisation des connexions HTTP

Uvicorn g√®re d√©j√† les keep-alive par d√©faut, mais on peut optimiser :

```python
uvicorn.run(
    ...,
    timeout_keep_alive=30,  # Garder les connexions ouvertes 30s
    limit_concurrency=10,   # Limiter les connexions simultan√©es
)
```

## üìä Configuration Recommand√©e (D√©veloppement Local)

### Configuration Minimale (Rapide √† impl√©menter) :

1. **Compression Gzip** ‚úÖ (1 ligne)
2. **Timeout r√©duit** ‚úÖ (1 ligne)
3. **Workers = 1** (garder single-threaded pour √©viter probl√®mes de partage m√©moire)

### Configuration Optimale (Production) :

1. **Compression Gzip** ‚úÖ
2. **Timeout 30s** ‚úÖ
3. **Workers = 2-4** (selon CPU)
4. **Cache simple** ‚úÖ
5. **Keep-alive optimis√©** ‚úÖ

## üîß Impl√©mentation Rapide

**Fichier √† modifier** : `rag_assistant_server.py`

**Lignes √† ajouter/modifier** :

1. **Import Gzip** (apr√®s ligne 13) :
```python
from fastapi.middleware.gzip import GZipMiddleware
```

2. **Ajouter middleware** (apr√®s ligne 976) :
```python
app.add_middleware(GZipMiddleware, minimum_size=1000)
```

3. **R√©duire timeout** (ligne 921) :
```python
timeout=30.0,  # Au lieu de 60.0
```

4. **Optimiser uvicorn** (lignes 1296-1298) :
```python
uvicorn.run(
    "rag_assistant_server:app",
    host="0.0.0.0",
    port=8711,
    timeout_keep_alive=30,
    limit_concurrency=10
)
```

## üìà Gains Attendus

- **Compression** : -60% taille r√©ponses ‚Üí -40% temps transfert
- **Timeout r√©duit** : -50% temps d'attente max
- **Keep-alive** : -20% overhead connexions
- **Total estim√©** : **-30 √† -50% latence per√ßue**

## ‚ö†Ô∏è Notes Importantes

1. **Workers multiples** : N√©cessite que les embeddings/metadata soient thread-safe ou recharg√©s par worker
2. **Cache** : Peut masquer des mises √† jour de donn√©es (TTL court recommand√©)
3. **Compression** : Augmente l√©g√®rement l'usage CPU mais r√©duit la bande passante

## üß™ Test de Performance

Pour mesurer l'am√©lioration :

```python
import time

@app.post("/rag-assistant", response_model=AssistantResponse)
def rag_assistant_endpoint(payload: AssistantRequest):
    start_time = time.time()
    try:
        # ... code existant ...
        response_time = time.time() - start_time
        print(f"[PERF] Temps de r√©ponse: {response_time:.2f}s")
        return result
    except Exception as e:
        response_time = time.time() - start_time
        print(f"[PERF] Erreur apr√®s {response_time:.2f}s: {e}")
        raise
```

