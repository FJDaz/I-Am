# ü§ñ Ce qui peut √™tre automatis√© vs n√©cessite intervention manuelle

**Date :** 2025-01-XX  
**Contexte :** Plan de g√©n√©ralisation I-Amiens √† tout le site

---

## ‚úÖ Ce que je peux faire automatiquement (sans intervention)

### Phase 1 : Refactoriser les Modules de D√©couverte

‚úÖ **Cr√©er `tools/discover_urls.py`**
- Code complet avec classe `URLDiscoverer`
- Toutes les strat√©gies (push-blocks, liens internes, navigation, sitemap)
- Gestion robots.txt et d√©lais

‚úÖ **Modifier `tools/rebuild_corpus.py`**
- Importer `URLDiscoverer`
- Remplacer `discover_push_blocks()` par appel √† `URLDiscoverer`
- Rendre `load_sources()` configurable

### Phase 2 : Configuration Multi-Sections

‚úÖ **Cr√©er `ML/data/site_sections.json`**
- Structure compl√®te avec toutes les sections
- Enfance activ√©e, autres d√©sactiv√©es par d√©faut
- Settings globaux configur√©s

‚úÖ **Cr√©er `tools/crawl_site_generalized.py`**
- Script complet de crawl g√©n√©ralis√©
- Utilise toutes les strat√©gies de d√©couverte
- G√®re multi-sections automatiquement

### Phase 3 : G√©n√©raliser le Scraping Dynamique

‚úÖ **Renommer et modifier `ML/scripts/Audit_Scrap_enfance.py` ‚Üí `crawl_dynamic.py`**
- G√©n√©raliser BASE_URL (param√®tre)
- Support multi-sections
- S√©lecteurs g√©n√©ralis√©s

### Phase 4 : R√©g√©n√©ration Embeddings

‚úÖ **Modifier `ML/embed_corpus.py`**
- Adapter pour charger `corpus_metadata_generalized.json`
- G√©n√©rer `corpus_embeddings_generalized.npy`

### Phase 5 : Adaptation Syst√®me RAG

‚úÖ **Modifier `Backend/rag_assistant_server.py` - Chemins**
- D√©tection automatique corpus_generalized si existe
- Fallback sur corpus actuel

‚úÖ **Modifier `Backend/rag_assistant_server.py` - Prompt syst√®me**
- G√©n√©raliser "Amiens Enfance" ‚Üí "Amiens"
- Ou prompt dynamique selon section d√©tect√©e

‚úÖ **Cr√©er structure lexiques multi-sections**
- Cr√©er fichiers `lexique_jeunesse.json`, etc. (vides ou templates)
- Adapter `load_lexicon()` pour chargement conditionnel

‚úÖ **Adapter `load_structured_data()`**
- Chargement conditionnel selon section
- Garder RPE/tarifs/√©coles seulement si Enfance

### Phase 5 : Optimisation Performance

‚úÖ **Cr√©er `Backend/cache.py`**
- Module cache m√©moire complet
- Dict Python avec TTL
- Fonctions get/set/clear

‚úÖ **Int√©grer cache dans `rag_assistant_endpoint()`**
- V√©rifier cache avant recherche RAG
- Sauvegarder r√©sultats dans cache

‚úÖ **Optimiser recherche RAG**
- R√©duire `top_k * 4` ‚Üí `top_k * 2`
- Cache embeddings de requ√™tes fr√©quentes
- Optimiser sch√©ma Whoosh

### Phase 6 : Documentation

‚úÖ **Cr√©er `docs/tutos/GUIDE_GENERALISATION_SITE.md`**
- Guide complet d'utilisation
- Exemples de configuration
- D√©pannage

‚úÖ **Mettre √† jour `docs/references/strategies-scraping-generalisation.md`**
- Documenter toutes les strat√©gies
- Exemples d'utilisation

---

## ‚ö†Ô∏è Ce qui n√©cessite intervention manuelle

### Tests et Validation

‚ùå **Test de r√©gression sur Enfance**
- N√©cessite : Ex√©cuter `crawl_site_generalized.py --section "Enfance"`
- N√©cessite : Comparer r√©sultats avec corpus actuel
- N√©cessite : Validation que tout fonctionne

‚ùå **Test sur nouvelles sections**
- N√©cessite : Activer section dans `site_sections.json`
- N√©cessite : Crawler et v√©rifier qualit√© segments
- N√©cessite : Tester recherche RAG sur questions

‚ùå **Test syst√®me RAG complet**
- N√©cessite : Tester recherche hybride multi-sections
- N√©cessite : V√©rifier performance latence
- N√©cessite : Validation qualit√© r√©ponses

### D√©cisions et Configuration

‚ùå **Tester Claude Haiku vs Sonnet**
- N√©cessite : D√©cision utilisateur (qualit√© vs rapidit√©)
- N√©cessite : Tests comparatifs avec √©chantillon questions
- N√©cessite : Validation que Haiku est acceptable

‚ùå **Activer sections dans `site_sections.json`**
- N√©cessite : D√©cision utilisateur (quelles sections activer)
- N√©cessite : V√©rification que les URLs existent

‚ùå **Remplir lexiques multi-sections**
- N√©cessite : Contenu sp√©cifique par section
- N√©cessite : Validation avec utilisateurs

### D√©ploiement

‚ùå **D√©ployer sur Railway**
- N√©cessite : Credentials Railway
- N√©cessite : Variables d'environnement
- N√©cessite : Test en production

‚ùå **R√©g√©n√©rer embeddings en production**
- N√©cessite : Upload nouveau corpus sur Railway
- N√©cessite : R√©g√©n√©rer embeddings (peut √™tre long)
- N√©cessite : Red√©marrer serveur

---

## üöÄ Plan d'Action Automatisable

### Ce que je peux faire maintenant (sans attendre)

1. ‚úÖ Cr√©er tous les fichiers de code (discover_urls.py, crawl_site_generalized.py, cache.py)
2. ‚úÖ Cr√©er fichiers de configuration (site_sections.json)
3. ‚úÖ Modifier fichiers existants (rebuild_corpus.py, rag_assistant_server.py, embed_corpus.py)
4. ‚úÖ Cr√©er documentation compl√®te
5. ‚úÖ G√©n√©raliser scraping dynamique (crawl_dynamic.py)

### Ce qui n√©cessite votre validation apr√®s

1. ‚ö†Ô∏è Tester le crawl sur Enfance (v√©rifier r√©gression)
2. ‚ö†Ô∏è Activer sections souhait√©es dans `site_sections.json`
3. ‚ö†Ô∏è Tester Claude Haiku et d√©cider
4. ‚ö†Ô∏è Valider qualit√© r√©ponses multi-sections
5. ‚ö†Ô∏è D√©ployer sur Railway

---

## üí° Recommandation

**Je peux lancer maintenant :**
- Tous les fichiers de code et configuration
- Toutes les modifications de code
- Toute la documentation

**Vous devrez ensuite :**
- Tester et valider
- Activer les sections souhait√©es
- D√©cider Claude Haiku vs Sonnet
- D√©ployer sur Railway

**Souhaitez-vous que je commence par cr√©er tous les fichiers automatiquement ?**

---

**Derni√®re mise √† jour :** 2025-01-XX

