#!/usr/bin/env python3
"""
Crawler g√©n√©ralis√© pour tout le site amiens.fr
Utilise toutes les strat√©gies de d√©couverte document√©es

Usage:
    python crawl_site_generalized.py [--section SECTION] [--all]
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Set, Optional

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from tools.discover_urls import URLDiscoverer, fetch_page
from tools.rebuild_corpus import parse_page, Segment, slugify

ROOT = Path(__file__).resolve().parents[1]
CONFIG_PATH = ROOT / "ML" / "data" / "site_sections.json"
OUTPUT_PATH = ROOT / "ML" / "data" / "corpus_metadata_generalized.json"


def load_sections_config() -> Dict:
    """Charge la configuration des sections"""
    if not CONFIG_PATH.exists():
        print(f"‚ùå Fichier de configuration non trouv√©: {CONFIG_PATH}")
        sys.exit(1)
    
    with CONFIG_PATH.open(encoding="utf-8") as f:
        return json.load(f)


def crawl_section(section_config: Dict, discoverer: URLDiscoverer,
                  max_pages: int = 200) -> List[Segment]:
    """
    Crawl une section compl√®te du site en utilisant toutes les strat√©gies
    
    Args:
        section_config: Configuration de la section
        discoverer: Instance de URLDiscoverer
        max_pages: Nombre maximum de pages √† crawler
    
    Returns:
        Liste de segments extraits
    """
    base_url = section_config["base_url"]
    pattern = section_config.get("pattern")
    categories = section_config.get("categories", [])
    section_name = section_config.get("name", "unknown")
    
    print(f"\n{'='*60}")
    print(f"üîç Crawling section: {section_name}")
    print(f"   URL: {base_url}")
    print(f"   Pattern: {pattern}")
    print(f"   Cat√©gories: {', '.join(categories)}")
    print(f"{'='*60}\n")
    
    all_segments: List[Segment] = []
    to_visit: List[str] = [base_url]
    visited: Set[str] = set()
    
    while to_visit and len(visited) < max_pages:
        url = to_visit.pop(0)
        
        if url in visited:
            continue
        
        visited.add(url)
        print(f"üìÑ [{len(visited)}/{max_pages}] {url}")
        
        try:
            # Parser la page et extraire les segments
            segments = parse_page(url, categories[0] if categories else section_name.lower())
            all_segments.extend(segments)
            print(f"   ‚úÖ {len(segments)} segments extraits")
            
            # D√©couvrir de nouvelles URLs avec toutes les strat√©gies
            soup = fetch_page(url)
            
            # Strat√©gie 1: Push-blocks (H2 ‚Üí URLs)
            push_urls = discoverer.discover_push_blocks(soup, url)
            if push_urls:
                print(f"   üîó Push-blocks: {len(push_urls)} URLs d√©couvertes")
                for new_url in push_urls:
                    if new_url not in visited and new_url not in to_visit:
                        to_visit.append(new_url)
            
            # Strat√©gie 2: Liens internes
            internal_urls = discoverer.discover_internal_links(soup, url, pattern)
            if internal_urls:
                print(f"   üîó Liens internes: {len(internal_urls)} URLs d√©couvertes")
                for new_url in internal_urls:
                    if new_url not in visited and new_url not in to_visit:
                        to_visit.append(new_url)
            
            # Strat√©gie 3: Navigation
            nav_urls = discoverer.discover_from_navigation(soup, url)
            if nav_urls:
                print(f"   üîó Navigation: {len(nav_urls)} URLs d√©couvertes")
                for new_url in nav_urls:
                    if new_url not in visited and new_url not in to_visit:
                        to_visit.append(new_url)
        
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur: {e}")
            continue
    
    print(f"\n‚úÖ Section {section_name}: {len(all_segments)} segments au total")
    return all_segments


def crawl_all_sections(config: Dict, section_name: Optional[str] = None) -> List[Segment]:
    """
    Crawl toutes les sections ou une section sp√©cifique
    
    Args:
        config: Configuration compl√®te
        section_name: Nom de la section √† crawler (None = toutes)
    
    Returns:
        Liste de tous les segments
    """
    settings = config.get("settings", {})
    discoverer = URLDiscoverer(
        delay=settings.get("delay_between_requests", 1.0),
        respect_robots=settings.get("respect_robots_txt", True)
    )
    
    all_segments: List[Segment] = []
    
    # Strat√©gie 0: Sitemap (si activ√©)
    if settings.get("use_sitemap", True):
        print("üó∫Ô∏è  D√©couverte via sitemap.xml...")
        sitemap_urls = discoverer.discover_from_sitemap()
        print(f"‚úÖ {len(sitemap_urls)} URLs d√©couvertes via sitemap")
    
    # Filtrer les sections
    sections = config.get("sections", [])
    if section_name:
        sections = [s for s in sections if s.get("name") == section_name]
        if not sections:
            print(f"‚ùå Section '{section_name}' non trouv√©e")
            return []
    
    # Filtrer les sections activ√©es
    sections = [s for s in sections if s.get("enabled", True)]
    
    # Trier par priorit√©
    sections.sort(key=lambda x: x.get("priority", 999))
    
    # Crawler chaque section
    for section in sections:
        max_pages = settings.get("max_pages_per_section", 200)
        segments = crawl_section(section, discoverer, max_pages)
        all_segments.extend(segments)
    
    return all_segments


def save_corpus(segments: List[Segment], output_path: Path):
    """Sauvegarde le corpus au format JSON"""
    corpus = [
        {
            "label": segment.label,
            "url": segment.url,
            "category": segment.category,
            "content": segment.content,
        }
        for segment in segments
    ]
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(corpus, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ Corpus sauvegard√©: {output_path}")
    print(f"   {len(corpus)} segments au total")


def main():
    parser = argparse.ArgumentParser(
        description="Crawler g√©n√©ralis√© pour amiens.fr"
    )
    parser.add_argument(
        "--section",
        type=str,
        help="Nom de la section √† crawler (ex: 'Enfance')"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Crawler toutes les sections activ√©es"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=str(OUTPUT_PATH),
        help="Chemin du fichier de sortie"
    )
    
    args = parser.parse_args()
    
    # Charger la configuration
    config = load_sections_config()
    
    # D√©terminer les sections √† crawler
    if args.all:
        section_name = None
    elif args.section:
        section_name = args.section
    else:
        # Par d√©faut: seulement les sections activ√©es
        section_name = None
    
    # Crawler
    print("üöÄ D√©marrage du crawl g√©n√©ralis√©\n")
    segments = crawl_all_sections(config, section_name)
    
    # Sauvegarder
    output_path = Path(args.output)
    save_corpus(segments, output_path)
    
    print(f"\n‚úÖ Crawl termin√©: {len(segments)} segments extraits")


if __name__ == "__main__":
    main()

